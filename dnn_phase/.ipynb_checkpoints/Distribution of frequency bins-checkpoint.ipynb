{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook explores the figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 2019-03-11 14:48:48 dataset:1127 found 2 files\n",
      "I 2019-03-11 14:48:52 dataset:1141 iteration size 21\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from convsep.dataset import LargeDatasetMulti\n",
    "from trainDNN import build_ca, train_auto\n",
    "from phase_transform import PhaseTransform\n",
    "from scipy.signal import blackmanharris, gaussian\n",
    "import os\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import theano\n",
    "import lasagne\n",
    "from lasagne.layers import ReshapeLayer,Layer\n",
    "from lasagne.init import Normal\n",
    "from lasagne.regularization import regularize_layer_params_weighted, l2, l1\n",
    "from lasagne.regularization import regularize_layer_params\n",
    "import convsep.util as util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "ld2 = LargeDatasetMulti(path_transform_in=\"results/features\", nsources=4,\n",
    "                    batch_size=16, batch_memory=200,\n",
    "                    time_context=11, overlap=75, nprocs=4,\n",
    "                    mult_factor_in=0.3, mult_factor_out=0.3,\n",
    "                    extra_features=True, model=\"p\")\n",
    "ld2.extra_feat_size = 2049\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nchannels:  2\n",
      "nsources:  4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "db = \"../DSD100subset/\"\n",
    "model=\"dsd_fft_1024\"\n",
    "nepochs = 40\n",
    "scale_factor = 0.3\n",
    "tt = PhaseTransform(frameSize=4096, hopSize=1024, sampleRate=44100, window=gaussian, std=0.4)\n",
    "\n",
    "# train_errs = train_auto(train=ld2, fun=build_ca, transform=tt,\n",
    "#                 outdir=os.path.join(db,'output',model),\n",
    "#                 testdir=os.path.join(db,'Mixtures'),\n",
    "#                 model=os.path.join(db,'models',\"model_\"+model+\".pkl\"),\n",
    "#                 num_epochs=nepochs,\n",
    "#                 scale_factor=scale_factor)\n",
    "\n",
    "fun = build_ca\n",
    "train = ld2\n",
    "load=False\n",
    "skip_train=False\n",
    "skip_sep=False\n",
    "chunk_size=60\n",
    "chunk_overlap=2\n",
    "nsamples=40\n",
    "batch_size=32\n",
    "batch_memory=50\n",
    "time_context=30\n",
    "overlap=25\n",
    "nprocs=4\n",
    "mult_factor_in=0.3\n",
    "mult_factor_out=0.3\n",
    "transform = tt\n",
    "outdir=os.path.join(db,'output',model)\n",
    "testdir=os.path.join(db,'Mixtures')\n",
    "model=os.path.join(db,'models',\"model_\"+model+\".pkl\")\n",
    "num_epochs=nepochs\n",
    "scale_factor=scale_factor\n",
    "\n",
    "\n",
    "amp = T.tensor4('amp')\n",
    "dt_ph = T.tensor4('dt_ph')\n",
    "df_ph = T.tensor4('df_ph')\n",
    "# input_var2 = T.tensor4('inputs')\n",
    "input_var = T.tensor4('inputs')\n",
    "input_mask = T.tensor4('input_mask')\n",
    "target_var = T.tensor4('targets')\n",
    "\n",
    "theano_rng = RandomStreams(128)\n",
    "\n",
    "eps=1e-12\n",
    "\n",
    "sources = ['vocals','bass','drums','other']\n",
    "\n",
    "nchannels = int(train.channels_in)\n",
    "nsources = int(train.channels_out/train.channels_in)\n",
    "\n",
    "print('nchannels: ', nchannels)\n",
    "print('nsources: ', nsources)\n",
    "\n",
    "input_size = int(float(transform.frameSize) / 2 + 1)\n",
    "\n",
    "rand_num = theano_rng.normal( size = (batch_size,nsources,time_context,input_size), avg = 0.0, std = 0.1, dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = fun(amp=amp, df_ph=df_ph, dt_ph=dt_ph,\n",
    "        batch_size=batch_size,time_context=time_context,\n",
    "        feat_size=input_size, nchannels=nchannels, nsources=nsources)\n",
    "\n",
    "if load:\n",
    "    params=load_model(model)\n",
    "    lasagne.layers.set_all_param_values(network,params)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "\n",
    "sourceall=[]\n",
    "errors_insts = []\n",
    "loss = 0\n",
    "\n",
    "sep_chann = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.squared_error(prediction, abs(target_var[:,:,5,:]))\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "you might consider using 'theano.shared(..., borrow=True)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f071093e057b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madadelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlosser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\lasagne\\updates.py\u001b[0m in \u001b[0;36madadelta\u001b[1;34m(loss_or_grads, params, learning_rate, rho, epsilon)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# accu: accumulate gradient magnitudes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m         accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n\u001b[1;32m--> 546\u001b[1;33m                              broadcastable=param.broadcastable)\n\u001b[0m\u001b[0;32m    547\u001b[0m         \u001b[1;31m# delta_accu: accumulate update magnitudes (recursively!)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         delta_accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\theano\\compile\\sharedvalue.py\u001b[0m in \u001b[0;36mshared\u001b[1;34m(value, name, strict, allow_downcast, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 var = ctor(value, name=name, strict=strict,\n\u001b[1;32m--> 268\u001b[1;33m                            allow_downcast=allow_downcast, **kwargs)\n\u001b[0m\u001b[0;32m    269\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_tag_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py36\\lib\\site-packages\\theano\\tensor\\sharedvar.py\u001b[0m in \u001b[0;36mtensor_constructor\u001b[1;34m(value, name, strict, allow_downcast, borrow, broadcastable, target)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcastable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbroadcastable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     return TensorSharedVariable(type=type,\n\u001b[1;32m---> 54\u001b[1;33m                                 \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mborrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                 \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: you might consider using 'theano.shared(..., borrow=True)'"
     ]
    }
   ],
   "source": [
    "params1 = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.adadelta(loss, params1)\n",
    "\n",
    "losser=[]\n",
    "\n",
    "if not skip_train:\n",
    "    logging.info(\"Training stage 1 (mse)...\")\n",
    "#     for epoch in range(num_epochs):\n",
    "#         train_err = 0\n",
    "#         train_batches = 0\n",
    "#         errs = np.zeros((nchannels,nsources))\n",
    "#         start_time = time.time()\n",
    "#         for batch in range(train.iteration_size):\n",
    "#             mag, target, features = train()\n",
    "#             df_ph, dt_ph = features[..., :]\n",
    "#             train_err += train_fn_mse(mag, target)\n",
    "#             errs += np.array(train_fn1(mag, df_ph, dt_ph, target))\n",
    "#             train_batches += 1\n",
    "\n",
    "#         logging.info(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "#             epoch + 1, num_epochs, time.time() - start_time))\n",
    "#         logging.info(\"  training loss:\\t\\t{:.6f}\".format(train_err/train_batches))\n",
    "#         for j in range(nchannels):\n",
    "#             for i in range(nsources):\n",
    "#                 logging.info(\"  training loss for \"+sources[i]+\" in mic \"+str(j)+\":\\t\\t{:.6f}\".format(errs[j][i]/train_batches))\n",
    "\n",
    "#         model_noILD = model[:-4] + '_noILD' + model[-4:]\n",
    "#         print('model_noILD: ', model_noILD)\n",
    "#         save_model(model_noILD,network)\n",
    "#         losser.append(train_err/train_batches)\n",
    "        \n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "#NEW ILD TRAINING---------------------------------------------------------\n",
    "\n",
    "    params=load_model(model_noILD)\n",
    "    lasagne.layers.set_all_param_values(network,params)\n",
    "    params1 = lasagne.layers.get_all_params(network,trainable=True)\n",
    "    updates = lasagne.updates.adadelta(loss,params1)\n",
    "    train_fn_ILD = theano.function([amp,target_var],loss,updates=updates,allow_input_downcast=True)\n",
    "\n",
    "    logging.info(\"Training stage 2 (ILD)...\")\n",
    "\n",
    "    for epoch in range(int(num_epochs/2)):\n",
    "\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for batch in range(train.iteration_size):\n",
    "            inputs,target = train()\n",
    "\n",
    "            train_err+=train_fn_ILD(inputs,target)\n",
    "            train_batches+=1\n",
    "\n",
    "        logging.info(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        logging.info(\"  training loss:\\t\\t{:.6f}\".format(train_err/train_batches))\n",
    "\n",
    "        save_model(model,network)\n",
    "        losser.append(train_err/train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, features = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 8, 1, 2049)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:,:,0:1,:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
